{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f413c938-0fc3-43e1-9f5a-228c4a7f807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.image import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd8b40c9-067b-463b-92cd-f7c8fd76096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"UCF-crime-frames-descriptions-partitions.csv\")\n",
    "train = data.loc[data[\"partition\"] == \"train\"]\n",
    "val = data.loc[data[\"partition\"] == \"validation\"]\n",
    "test = data.loc[data[\"partition\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "575203f0-b046-442e-a8d1-6bae1666b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"Normal\": 0,\n",
    "          \"Abuse\": 1,\n",
    "          \"Arrest\": 2,\n",
    "          \"Arson\": 3,\n",
    "          \"Burglary\": 4,\n",
    "          \"Explosion\": 5,\n",
    "          \"Fighting\": 6,\n",
    "          \"RoadAccidents\": 7,\n",
    "          \"Shooting\": 8,\n",
    "          \"Vandalism\": 9\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb434bf8-ad3a-44b4-8530-a4fa8a234711",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.densenet.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "462d040b-dd6a-4d96-b1eb-d7e457a2d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_seq(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, directorys, batch_size, path):\n",
    "        self.x = list(x_set)\n",
    "        self.y = list(y_set)\n",
    "        self.y = np.array([labels[lbl] for lbl in self.y])\n",
    "        self.batch_size = batch_size\n",
    "        self.dirs = list(directorys)\n",
    "        self.path = path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low = idx * self.batch_size\n",
    "        # Cap upper bound at array length; the last batch may be smaller\n",
    "        # if the total number of items is not a multiple of batch size.\n",
    "        high = min(low + self.batch_size, len(self.x))\n",
    "        batch_x = self.x[low:high]\n",
    "        batch_y = self.y[low:high]       \n",
    "        batch_dirs = self.dirs[low:high]\n",
    "        images = []\n",
    "        for d, file in zip(batch_dirs, batch_x):\n",
    "            im = tf.keras.preprocessing.image.load_img(self.path + d + \"/\" + file)\n",
    "            im = tf.keras.preprocessing.image.img_to_array(im)            \n",
    "            im = resize(im, (64, 64), method=\"bilinear\")            \n",
    "            images.append(im)            \n",
    "        images = self.pre_process_img(np.array(images))\n",
    "        return images, np.array(batch_y)\n",
    "    \n",
    "    def pre_process_img(self, X):\n",
    "        return preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b51f9f06-0d32-49e2-9e02-3da8ef86f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"d:/data/UCF-crime/Anomaly-Videos-Frames/\"\n",
    "X = img_seq(train[\"frame\"], train[\"class\"], train[\"directory\"], 64, path)\n",
    "V = img_seq(val[\"frame\"], val[\"class\"], val[\"directory\"], 64, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1578fbe-e692-491d-99a3-1491543150b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.applications.DenseNet121(input_shape=(64, 64, 3),\n",
    "                                          include_top=False,\n",
    "                                          weights=\"imagenet\")\n",
    "    \n",
    "    \n",
    "inputs = tf.keras.layers.Input(shape=(64, 64,3))\n",
    "x = dense(inputs)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "#x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
    "#x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.4) (x)\n",
    "x = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"classification\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs = x)\n",
    "dense.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27bb7c35-bd43-4f62-aa56-9fafd2679378",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics = [\"acc\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0564f0b6-15d5-4ff6-b100-17d55edc33e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1939/1939 [==============================] - 634s 327ms/step - loss: 0.8884 - acc: 0.7376 - val_loss: 1.3050 - val_acc: 0.6620\n",
      "Epoch 2/10\n",
      "1939/1939 [==============================] - 634s 327ms/step - loss: 0.7131 - acc: 0.7822 - val_loss: 1.3503 - val_acc: 0.6654\n",
      "Epoch 3/10\n",
      "1939/1939 [==============================] - 632s 326ms/step - loss: 0.6025 - acc: 0.8102 - val_loss: 1.4315 - val_acc: 0.6581\n",
      "Epoch 4/10\n",
      "1939/1939 [==============================] - 635s 327ms/step - loss: 0.5278 - acc: 0.8265 - val_loss: 1.4262 - val_acc: 0.6596\n",
      "Epoch 5/10\n",
      "1939/1939 [==============================] - 630s 325ms/step - loss: 0.4783 - acc: 0.8400 - val_loss: 1.6345 - val_acc: 0.6635\n",
      "Epoch 6/10\n",
      "1939/1939 [==============================] - 631s 326ms/step - loss: 0.4386 - acc: 0.8510 - val_loss: 1.5396 - val_acc: 0.6616\n",
      "Epoch 7/10\n",
      "1939/1939 [==============================] - 636s 328ms/step - loss: 0.4013 - acc: 0.8614 - val_loss: 1.6177 - val_acc: 0.6586\n",
      "Epoch 8/10\n",
      "1939/1939 [==============================] - 636s 328ms/step - loss: 0.3751 - acc: 0.8695 - val_loss: 1.6244 - val_acc: 0.6472\n",
      "Epoch 9/10\n",
      "1939/1939 [==============================] - 629s 325ms/step - loss: 0.3487 - acc: 0.8763 - val_loss: 1.6522 - val_acc: 0.6606\n",
      "Epoch 10/10\n",
      "1939/1939 [==============================] - 631s 325ms/step - loss: 0.3305 - acc: 0.8810 - val_loss: 1.7961 - val_acc: 0.6537\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, validation_data=V, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dbc6806c-b76b-4235-95b9-26a1149f1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dense.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ba3398d-368e-40f2-9b2d-354a77d17a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bda00ecd-5304-4ccb-b116-5c15463a45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.97      0.79     12893\n",
      "           1       0.14      0.03      0.05       646\n",
      "           2       0.01      0.00      0.00      1321\n",
      "           3       0.02      0.02      0.02       106\n",
      "           4       0.43      0.03      0.06      1767\n",
      "           5       0.47      0.05      0.09       388\n",
      "           6       0.01      0.00      0.00      1132\n",
      "           7       0.00      0.00      0.00        12\n",
      "           8       0.00      0.00      0.00        44\n",
      "           9       0.00      0.00      0.00      1019\n",
      "\n",
      "    accuracy                           0.65     19328\n",
      "   macro avg       0.17      0.11      0.10     19328\n",
      "weighted avg       0.50      0.65      0.54     19328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(V.y[0:19328], tf.argmax(val_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7058ab3-d099-4cda-867b-034ef8767c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6611cbf5-5091-4f25-84f8-122754fbbe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     84646\n",
      "           1       0.71      0.77      0.74      2593\n",
      "           2       0.90      0.97      0.93     10454\n",
      "           3       0.93      0.93      0.93      4984\n",
      "           4       0.91      0.89      0.90      8211\n",
      "           5       0.93      0.74      0.83      2138\n",
      "           6       0.85      0.83      0.84      6178\n",
      "           7       0.29      0.46      0.36       689\n",
      "           8       0.82      0.37      0.51      1557\n",
      "           9       0.79      0.74      0.76      2646\n",
      "\n",
      "    accuracy                           0.91    124096\n",
      "   macro avg       0.81      0.76      0.77    124096\n",
      "weighted avg       0.91      0.91      0.91    124096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(X.y[0:len(y_pred)], tf.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29c018-0d85-4d15-9a2e-c66fa6b18c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
